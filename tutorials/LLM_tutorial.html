<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Tutorial LLM capabilities of DLLMForge &#8212; dllmforge  documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=686e5160" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Open Source RAG Pipeline Tutorial" href="RAG_tutorial.html" />
    <link rel="prev" title="DLLMForge Documentation" href="../index.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="tutorial-llm-capabilities-of-dllmforge">
<h1>Tutorial LLM capabilities of DLLMForge<a class="headerlink" href="#tutorial-llm-capabilities-of-dllmforge" title="Link to this heading">¶</a></h1>
<p>This tutorial demonstrates how to use a simple LLM to ask questions using DLLMForge.
There are different API’s available to use LLMs from OpenAI, Mistral, AZUREOpenAI or Deltares hosted models.
There are three different ways to use LLMs using:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../dllmforge.html#dllmforge.llamaindex_api.LlamaIndexAPI" title="dllmforge.llamaindex_api.LlamaIndexAPI"><code class="xref py py-class docutils literal notranslate"><span class="pre">LlamaIndexAPI</span></code></a> - Containing LlamaIndex framework integration with OpenAI, AZUREOpenAI and Mistral models.</p></li>
<li><p><a class="reference internal" href="../dllmforge.html#dllmforge.langchain_api.LangchainAPI" title="dllmforge.langchain_api.LangchainAPI"><code class="xref py py-class docutils literal notranslate"><span class="pre">LangchainAPI</span></code></a> - Containing LangChain framework integration with OpenAI, AZUREOpenAI and Mistral models.</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DeltaresOllamaLLM</span></code> - Containing Deltares hosted models.</p></li>
</ul>
<p>For the OpenAI, Mistral a .env file is needed with the API keys, these are specified in the following section.
For the Deltares hosted models, no API key is needed, but you need to be on the Deltares network or VPN.</p>
<section id="environment-setup">
<h2>Environment Setup<a class="headerlink" href="#environment-setup" title="Link to this heading">¶</a></h2>
<p>To use the OpenAI model the following environment variables are needed in a .env file:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>OPENAI_API_KEY=your_openai_api_key
OPENAI_MODEL_NAME=gpt-4 # or other available models
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>AZURE_OPENAI_API_KEY=your_azure_openai_api_key
AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint
AZURE_OPENAI_DEPLOYMENT_NAME=your_deployment_name
AZURE_OPENAI_API_VERSION=your_api_version
AZURE_OPENAI_MODEL_NAME=gpt-4 # or other available models
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>MISTRAL_API_KEY=your_mistral_api_key
MISTRAL_MODEL_NAME=mistral-7b-instruct-v0.1 # or other available models
</pre></div>
</div>
</section>
<section id="initialize-llm">
<h2>Initialize LLM<a class="headerlink" href="#initialize-llm" title="Link to this heading">¶</a></h2>
<p>First, we initialize the LLM using one of the available API’s.</p>
<p>Using LlamaIndex API with OpenAIAPI</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">dllmforge.llamaindex_api</span><span class="w"> </span><span class="kn">import</span> <span class="n">LlamaIndexAPI</span>

<span class="c1"># Initialize OpenAI API</span>
<span class="n">api_llama_openai</span> <span class="o">=</span> <span class="n">LlamaIndexAPI</span><span class="p">(</span><span class="n">model_provider</span><span class="o">=</span><span class="s2">&quot;openai&quot;</span><span class="p">)</span>
<span class="n">api_llama_mistral</span> <span class="o">=</span> <span class="n">LlamaIndexAPI</span><span class="p">(</span><span class="n">model_provider</span><span class="o">=</span><span class="s2">&quot;mistral&quot;</span><span class="p">)</span>
<span class="n">api_llama_azure</span> <span class="o">=</span> <span class="n">LlamaIndexAPI</span><span class="p">(</span><span class="n">model_provider</span><span class="o">=</span><span class="s2">&quot;azure-openai&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Using LangChain API with OpenAIAPI</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">dllmforge.langchain_api</span><span class="w"> </span><span class="kn">import</span> <span class="n">LangchainAPI</span>

<span class="c1"># Initialize OpenAI API</span>
<span class="n">api_langchain_openai</span> <span class="o">=</span> <span class="n">LangchainAPI</span><span class="p">(</span><span class="n">model_provider</span><span class="o">=</span><span class="s2">&quot;openai&quot;</span><span class="p">)</span>
<span class="n">api_langchain_mistral</span> <span class="o">=</span> <span class="n">LangchainAPI</span><span class="p">(</span><span class="n">model_provider</span><span class="o">=</span><span class="s2">&quot;mistral&quot;</span><span class="p">)</span>
<span class="n">api_langchain_azure</span> <span class="o">=</span> <span class="n">LangchainAPI</span><span class="p">(</span><span class="n">model_provider</span><span class="o">=</span><span class="s2">&quot;azure-openai&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Using Deltares hosted models</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">dllmforge.LLMs.Deltares_LLMs</span><span class="w"> </span><span class="kn">import</span> <span class="n">DeltaresOllamaLLM</span>

<span class="c1"># Initialize Deltares hosted model</span>
<span class="n">base_url</span> <span class="o">=</span> <span class="s2">&quot;https://chat-api.directory.intra&quot;</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;llama3.1:70b&quot;</span>  <span class="c1"># or other available models</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">DeltaresOllamaLLM</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="n">base_url</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="using-the-llm-to-ask-questions">
<h2>Using the LLM to ask questions<a class="headerlink" href="#using-the-llm-to-ask-questions" title="Link to this heading">¶</a></h2>
<p>Now we can use the initialized LLM to ask questions.
All of the classes have a method <cite>chat_completions</cite> to ask a question to the LLM.</p>
<p>The method takes a dictionary of messages as input and returns the response from the LLM.
The dictionary of messages should contain a list of messages with the role and content.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the messages</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the capital of France?&quot;</span><span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
<p>Then the messages can be passed to the <cite>chat_completions</cite> method of the different API’s.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Using LlamaIndex API with OpenAI</span>
<span class="n">response_llama_openai</span> <span class="o">=</span> <span class="n">api_llama_openai</span><span class="o">.</span><span class="n">chat_completion</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LlamaIndex OpenAI Response:&quot;</span><span class="p">,</span> <span class="n">response_llama_openai</span><span class="p">)</span>

<span class="c1"># Using LangChain API with OpenAI</span>
<span class="n">response_langchain_openai</span> <span class="o">=</span> <span class="n">api_langchain_openai</span><span class="o">.</span><span class="n">chat_completion</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LangChain OpenAI Response:&quot;</span><span class="p">,</span> <span class="n">response_langchain_openai</span><span class="p">)</span>

<span class="c1"># Using Deltares hosted model</span>
<span class="n">response_deltares</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">chat_completion</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Deltares Model Response:&quot;</span><span class="p">,</span> <span class="n">response_deltares</span><span class="p">)</span>
</pre></div>
</div>
<p>This tutorial demonstrates how to use a simple LLM to ask questions using DLLMForge.
There are different API’s available to use LLMs from OpenAI, Mistral, AZUREOpenAI or Deltares hosted models.</p>
</section>
<section id="footnote-about-deltares-hosted-models">
<h2>Footnote about Deltares hosted models<a class="headerlink" href="#footnote-about-deltares-hosted-models" title="Link to this heading">¶</a></h2>
<p>Note that for the Deltares hosted models you can also define the temperature and max_tokens parameters in the <cite>chat_completions</cite> method.</p>
<p>The temperature parameter (between 0 and 1) controls the randomness of the output.
A low temperature (closer to 0) makes the output more focused, deterministic, and repetitive, as the model sticks to the most probable words,
making it ideal for tasks like factual summaries. A high temperature (above 1) makes the output more random, creative, and varied, as the model
is more likely to choose less likely words, which is better for creative writing or brainstorming.</p>
<p>The max_tokens parameter controls the maximum length of the output in tokens. It can be used to limit the response length and ensure that the
output fits within specific constraints.</p>
<p>To turn off the thinking process of the model, you can add the “/no_think” flag to the messages.</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">dllmforge</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Tutorials:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tutorial LLM capabilities of DLLMForge</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#environment-setup">Environment Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#initialize-llm">Initialize LLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-the-llm-to-ask-questions">Using the LLM to ask questions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#footnote-about-deltares-hosted-models">Footnote about Deltares hosted models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="RAG_tutorial.html">Open Source RAG Pipeline Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial_simple_agent.html">Building a Simple Agent with DLLMForge</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial_advanced_agent.html">Tutorial: Advanced Water Management Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="Information_extraction_tutorial.html">Information Extraction with LLMs Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Background:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../background/LLM_explained.html">LLM Explained</a></li>
<li class="toctree-l1"><a class="reference internal" href="../background/RAGS_explained.html">Retrieval-Augmented Generation (RAG)</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../_autosummary/dllmforge.html">dllmforge</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="../index.html" title="previous chapter">DLLMForge Documentation</a></li>
      <li>Next: <a href="RAG_tutorial.html" title="next chapter">Open Source RAG Pipeline Tutorial</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2023-2025, dllmforge team.
      
    </div>

    

    
  </body>
</html>
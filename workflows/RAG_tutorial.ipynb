{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dbe44af",
   "metadata": {},
   "source": [
    "# Open Source RAG Pipeline Tutorial\n",
    "\n",
    "This tutorial demonstrates how to build a complete **Retrieval-Augmented Generation (RAG)** pipeline using open source components from the DLLMForge library. \n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "The RAG pipeline consists of several key components:\n",
    "\n",
    "1. **Document Loading**: Load PDF documents and extract text\n",
    "2. **Text Chunking**: Split documents into manageable chunks\n",
    "3. **Embedding Generation**: Create vector embeddings using open source models\n",
    "4. **Vector Storage**: Store embeddings in a FAISS vector database\n",
    "5. **Retrieval**: Find relevant document chunks for queries\n",
    "6. **Generation**: Generate answers using an open source LLM\n",
    "7. **Evaluation**: Assess the quality of the RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3397ea94",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Modules\n",
    "\n",
    "Start by importing all necessary components for the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f22565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "INFO:faiss:Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    }
   ],
   "source": [
    "from dllmforge.rag_embedding_open_source import LangchainHFEmbeddingModel\n",
    "from dllmforge.rag_evaluation import RAGEvaluator\n",
    "from dllmforge.LLMs.Deltares_LLMs import DeltaresOllamaLLM\n",
    "from dllmforge.rag_preprocess_documents import PDFLoader, TextChunker\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from pathlib import Path\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f0b44",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Embedding Model\n",
    "\n",
    "Create an embedding model using open source HuggingFace transformers.\n",
    "\n",
    "The `LangchainHFEmbeddingModel` class supports any HuggingFace sentence transformer model and provides:\n",
    "- Automatic model downloading and caching\n",
    "- Batch embedding for efficient processing\n",
    "- Input validation for embeddings\n",
    "\n",
    "We use the multilingual-e5-large model here, but you can experiment with others from the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n",
    "\n",
    "**Note:** Bigger models perform better but require more resources and take longer to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3449763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLMs\\DLLMForge\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embedding model\n",
    "# Default model: \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = LangchainHFEmbeddingModel(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5472511d",
   "metadata": {},
   "source": [
    "## Step 3: Download Sample Documents\n",
    "\n",
    "For this tutorial, you'll need some PDF documents to use as your knowledge base.\n",
    "\n",
    "**Example:** Download the schemaGAN paper from [Science Direct](https://www.sciencedirect.com/science/article/pii/S0266352X25001260).\n",
    "\n",
    "Place the PDF in a folder named `documents` in your working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a50583",
   "metadata": {},
   "source": [
    "## Step 4: Load and Process Documents\n",
    "\n",
    "Two important parameters to consider when chunking documents:\n",
    "\n",
    "- **`chunk_size`**: The maximum size of each text chunk (in characters). Smaller chunks may improve retrieval performance but increase the number of chunks.\n",
    "- **`overlap_size`**: The number of overlapping characters between chunks. Overlapping chunks help preserve context but may increase redundancy.\n",
    "\n",
    "Now let's load PDF documents and create text chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03af4c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 107 chunks from SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf.\n",
      "Total embeddings generated: 107\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing PDF documents\n",
    "data_dir = Path(r'..\\..\\documents')\n",
    "pdfs = list(data_dir.glob(\"*.pdf\"))\n",
    "\n",
    "# Initialize document loader and chunker\n",
    "loader = PDFLoader()\n",
    "chunker = TextChunker(chunk_size=1000, overlap_size=200)\n",
    "\n",
    "global_embeddings = []\n",
    "metadatas = []\n",
    "\n",
    "# Process each PDF file\n",
    "for pdf_path in pdfs:\n",
    "    # Load the PDF document\n",
    "    pages, file_name, metadata = loader.load(pdf_path)\n",
    "    \n",
    "    # Create chunks with overlap for better context preservation\n",
    "    chunks = chunker.chunk_text(pages, file_name, metadata)\n",
    "    \n",
    "    # Generate embeddings for chunks\n",
    "    chunk_embeddings = model.embed(chunks)\n",
    "    \n",
    "    # Store embeddings and metadata\n",
    "    global_embeddings.extend(chunk_embeddings)\n",
    "    metadatas.extend([chunk[\"metadata\"] for chunk in chunks])\n",
    "    \n",
    "    print(f\"Embedded {len(chunk_embeddings)} chunks from {file_name}.\")\n",
    "\n",
    "print(f\"Total embeddings generated: {len(global_embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1302b6",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "Embedded ... chunks from ...pdf\n",
    "Total embeddings generated: ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d375a6e1",
   "metadata": {},
   "source": [
    "## Step 5: Create Vector Store\n",
    "\n",
    "Set up a FAISS vector store for efficient similarity search. FAISS is a library for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "**Alternative Index Types:**\n",
    "- `IndexFlatL2`: Exact L2 distance (slower but accurate)\n",
    "- `IndexFlatIP`: Inner product similarity\n",
    "- `IndexIVFFlat`: Faster approximate search for large datasets\n",
    "\n",
    "Other vector stores like MongoDB or Weaviate can also be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f0272d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created with 107 embeddings\n"
     ]
    }
   ],
   "source": [
    "# Get embedding dimension\n",
    "embedding_dim = len(global_embeddings[0][\"text_vector\"])\n",
    "\n",
    "# Create FAISS index for L2 (Euclidean) distance\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Initialize vector store\n",
    "vector_store = FAISS(\n",
    "    embedding_function=model.embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "# Add embeddings to the vector store\n",
    "for chunk, meta in zip(global_embeddings, metadatas):\n",
    "    vector_store.add_texts(\n",
    "        texts=[chunk[\"chunk\"]],\n",
    "        metadatas=[meta],\n",
    "        ids=[chunk[\"chunk_id\"]],\n",
    "        embeddings=[chunk[\"text_vector\"]]\n",
    "    )\n",
    "\n",
    "print(f\"Vector store created with {len(global_embeddings)} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c4c16",
   "metadata": {},
   "source": [
    "## Step 6: Test Retrieval\n",
    "\n",
    "Let's test the vector store with a sample query to see if it retrieves relevant chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0184d8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query result: [(Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i37', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content=', adhering \\nto best practices in the field of GAN training. During training, the \\nDiscriminator‚Äôs parameters are adjusted based on the results from the \\nloss function just like in the Generator.\\n2.4. The combined cGAN architecture\\nFor the compilation of the combined Generator and Discriminator \\nnetworks into the schemaGAN model, the Adam optimiser was also \\nused with a learning rate of 0.0002 and a beta value of 0.5 (Goodfellow \\net al., 2014; Salimans et al., 2016).\\nThe complete schemaGAN model boasts a total of 78,172,226 pa-\\nrameters representing the weights and biases of the network‚Äôs layers. \\nFrom these, a total of 67,003,137 are trainable parameters adjusted \\nduring the training process to optimise the model‚Äôs performance. The \\nremaining 11,169,089 parameters are non-trainable and contribute to \\nthe structure and functionality of the network without being influenced \\nby training.\\n2.5.'), np.float32(0.7891704)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i32', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='F.A. Campos Montero et al.\\nFig. 2. SchemaGAN Generator architecture based on a U-Net framework.\\nEach decoder block receives a skip connection from the corresponding \\nencoder block‚Äôs output, aiding in localising and reconstructing details \\nin the output (Ronneberger et al., 2015). The final layer is a transposed \\nConv2D layer using a 4 √ó 4 kernel and a stride of 2 √ó 2, which upscales \\nthe feature maps back to the original image size of 512 √ó 32 pixels with \\na \\ue00f\\ue009\\ue00e‚Ñé activation function. The output is an image of size 512 √ó 32 \\npixels with pixel values ranging from ‚àí1 to 1.\\nThroughout training, the weight parameters within the Generator \\nare iteratively adjusted based on the results from the objective func-\\ntion, optimising its ability to generate subsurface schematisations that \\nclosely align with the desired output.\\n2.3. The discriminator architecture\\nThe Discriminator functions as a combination of a CNN and a \\npatch-based discriminator, as proposed by Demir and Unal (2018).'), np.float32(0.8042669)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i39', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='F.A. Campos Montero et al.\\nAfter experimentation with different loss functions and weighting \\nfactors, the combined approach outlined in Eq. (1) with a \\ue001 value of 100 \\nwas selected as the optimal architecture. This choice implies a weight \\nratio of 1:100 between the BCE conditional GAN loss and the MAE loss. \\nBy prioritising the MAE loss, the generated schematisations maintain \\nfidelity to the input data while also exhibiting realistic features and \\nstructures (Isola et al., 2017).\\nA database of 16,000/4,000/4,000 cross-sections was created for \\nthe training, testing, and validation of schemaGAN, and for each sam-\\nple, the corresponding CPT image was generated. All the intensive \\ncomputation tasks were performed on DelftBlue supercomputer (DHPC, \\n2022) as detailed in the Section 4.\\n3.'), np.float32(0.8146047)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i85', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='Galanti, T., Benaim, S., Wolf, L., 2021. Evaluation metrics for conditional \\nimage generation. Int. J. Comput. Vis. 129 (5), 1712‚Äì1731. http://dx.doi.org/10.\\n1007/s11263-020-01424-w.\\nBorji, A., 2018. Pros and Cons of GAN Evaluation Measures. arXiv:1802.03446.\\nBorji, A., 2022. Pros and cons of GAN evaluation measures: New developments. Comput. \\nVis. Image Underst. 215, 103329. http://dx.doi.org/10.1016/j.cviu.2021.103329.\\nBowles, C., Chen, L., Guerrero, R., Bentley, P., Gunn, R., Hammers, A., Dickie, D.A., \\nHern√°ndez, M.V., Wardlaw, J., Rueckert, D., 2018. GAN Augmentation: augmenting \\ntraining data using generative adversarial networks. arXiv:1810.10863.\\nCampos Montero, F.A., 2023. Deep Learning for Geotechnical Engineering, The Effec-\\ntiveness of Generative Adversarial Networks in Subsoil Schematization (Master‚Äôs \\nthesis). Delft University of Technology, Delft.\\nCampos Montero, F., 2024. SchemaGAN: Dataset and pre-trained model for geotechnical \\nschematisation.'), np.float32(0.817944)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i11', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='type \\nof DL model consisting of two neural networks, which are trained \\nsimultaneously in a competitive setting to learn the underlying distri-\\nbution of the training data (Goodfellow et al., 2014). Unlike previous \\nmethods, GANs can generate new data points that closely mirror the \\nexisting data (Creswell et al., 2018; Goodfellow et al., 2020), a critical \\naspect for accurately capturing the complex relationships in subsurface \\nschematisation, where data is sparse and heterogeneous.'), np.float32(0.8270797))]\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      "Score: 0.7891703844070435\n",
      "Content: , adhering \n",
      "to best practices in the field of GAN training. During training, the \n",
      "Discriminator‚Äôs parameters are adjusted based on the results from the \n",
      "loss function just like in the Generator.\n",
      "2.4. ...\n",
      "Metadata: {'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}\n",
      "--------------------------------------------------------------------------------\n",
      "Result 2:\n",
      "Score: 0.8042669296264648\n",
      "Content: F.A. Campos Montero et al.\n",
      "Fig. 2. SchemaGAN Generator architecture based on a U-Net framework.\n",
      "Each decoder block receives a skip connection from the corresponding \n",
      "encoder block‚Äôs output, aiding in ...\n",
      "Metadata: {'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}\n",
      "--------------------------------------------------------------------------------\n",
      "Result 3:\n",
      "Score: 0.8146046996116638\n",
      "Content: F.A. Campos Montero et al.\n",
      "After experimentation with different loss functions and weighting \n",
      "factors, the combined approach outlined in Eq. (1) with a ÓÄÅ value of 100 \n",
      "was selected as the optimal arch...\n",
      "Metadata: {'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}\n",
      "--------------------------------------------------------------------------------\n",
      "Result 4:\n",
      "Score: 0.8179439902305603\n",
      "Content: Galanti, T., Benaim, S., Wolf, L., 2021. Evaluation metrics for conditional \n",
      "image generation. Int. J. Comput. Vis. 129 (5), 1712‚Äì1731. http://dx.doi.org/10.\n",
      "1007/s11263-020-01424-w.\n",
      "Borji, A., 2018. ...\n",
      "Metadata: {'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}\n",
      "--------------------------------------------------------------------------------\n",
      "Result 5:\n",
      "Score: 0.827079713344574\n",
      "Content: type \n",
      "of DL model consisting of two neural networks, which are trained \n",
      "simultaneously in a competitive setting to learn the underlying distri-\n",
      "bution of the training data (Goodfellow et al., 2014). U...\n",
      "Metadata: {'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Query the vector store directly\n",
    "query_embedding = vector_store.similarity_search_with_score(\n",
    "    query=\"Size of images for schema GAN\", \n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"Query result:\", query_embedding)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Each result contains (Document, similarity_score)\n",
    "for i, (doc, score) in enumerate(query_embedding, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"Score: {score}\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a5602c",
   "metadata": {},
   "source": [
    "## Step 7: Initialize the LLM\n",
    "\n",
    "Set up the open source language model using Ollama. We'll use the Qwen3 model in this example.\n",
    "\n",
    "**Note:** Make sure you're on the Deltares network or VPN to access the Deltares hosted models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6936d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ollama LLM\n",
    "llm = DeltaresOllamaLLM(\n",
    "    base_url=\"https://chat-api.directory.intra\",  \n",
    "    model_name=\"qwen3:latest\",  # Or another available model\n",
    "    temperature=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d87977",
   "metadata": {},
   "source": [
    "## Step 8: Create Retriever and Generate Answers\n",
    "\n",
    "Set up the retriever with similarity thresholds and generate answers to questions using the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b336d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Size of images produced by schemaGAN? give me the answer in axb format\n",
      "\n",
      "Answer: <think>\n",
      "Okay, the user is asking about the size of images produced by schemaGAN, specifically in axb format. Let me check the provided context documents.\n",
      "\n",
      "Looking through the documents, the first one (i32) mentions that the final layer of the Generator uses a transposed Conv2D layer to upscale feature maps back to the original image size of 512 √ó 32 pixels. Another document (i34) also refers to input images sized 512 √ó 32 √ó 1. Additionally, the Discriminator's input is mentioned as 512 √ó 32 √ó 1. \n",
      "\n",
      "So, it seems consistent that the image size is 512 pixels in width and 32 pixels in height. The user wants the answer in axb format, which typically represents width √ó height. Therefore, the answer should be 512√ó32.\n",
      "</think>\n",
      "\n",
      "The images produced by schemaGAN have a size of **512√ó32** pixels. This is explicitly mentioned in the context regarding the Generator's final layer upsampling to the original image size of 512 √ó 32 pixels.\n"
     ]
    }
   ],
   "source": [
    "# Create retriever with similarity threshold\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        \"score_threshold\": 0.1,  # Minimum similarity score\n",
    "        \"k\": 10  # Maximum number of documents to retrieve\n",
    "    },\n",
    ")\n",
    "\n",
    "# Generate answer using RAG\n",
    "question = \"Size of images produced by schemaGAN? give me the answer in axb format\"\n",
    "chat_result = llm.ask_with_retriever(question, retriever)\n",
    "answer = chat_result.generations[0].message.content\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b64e060",
   "metadata": {},
   "source": [
    "The answer should be relevant to the question based on the retrieved documents. If the answer is not satisfactory, consider:\n",
    "- Refining the query\n",
    "- Adjusting the retriever parameters\n",
    "- Using a different embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51328be",
   "metadata": {},
   "source": [
    "## Step 9: Evaluate the RAG System\n",
    "\n",
    "Use the built-in evaluation framework to assess RAG performance with multiple test questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f6d29fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting RAG evaluation...\n",
      "  üìä Evaluating context relevancy...\n",
      "  üìä Evaluating faithfulness...\n",
      "  üìä Evaluating answer relevancy...\n",
      "  üìä Evaluating context recall...\n",
      "Question: Size of images produced by schemaGAN?\n",
      "RAGAS Score: 0.285\n",
      "Answer Relevancy: 0.950\n",
      "Faithfulness: 0.500\n",
      "Context Recall: 1.000\n",
      "Context Relevancy: 0.100\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLMs\\DLLMForge\\.venv\\lib\\site-packages\\langchain_core\\vectorstores\\base.py:1082: UserWarning: Relevance scores must be between 0 and 1, got [(Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i28', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='or architecture\\nThe Generator employs convolutional neural networks (CNNs), \\nspecifically a modified U-Net architecture, due to their efficacy in \\nhandling high-dimensional data (LeCun et al., 2015). U-Net is a neural \\nnetwork architecture commonly used for image segmentation tasks, \\ncharacterised by its encoder‚Äìdecoder structure with skip connections in \\na symmetric structure (Ronneberger et al., 2015), facilitating the con-\\nveyance of both local and global structural information for generating \\nhigh-quality images.\\nFig. 2 illustrates the general architecture of the Generator, which \\ncomprises regular and irregular encoder and decoder blocks arranged \\nin a U-Net framework (Ronneberger et al., 2015).\\nBeginning with CPT-like images of dimensions 512 √ó 32 the en-\\ncoder progressively downsamples the images through eight sequential \\nblocks.'), np.float32(0.16820514)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i38', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='optimise the model‚Äôs performance. The \\nremaining 11,169,089 parameters are non-trainable and contribute to \\nthe structure and functionality of the network without being influenced \\nby training.\\n2.5. Training and validation of the model\\nThe final model undergoes training for 200 epochs, where each \\nepoch represents one complete pass through the entire dataset. During \\ntraining, a batch size of 1 is utilised, meaning that each training \\niteration involves processing a single sample from the dataset. This \\napproach mitigates mode collapse, allows for precise gradient up-\\ndates, conserves memory, and is known for faster convergence in the \\nmodels (Goodfellow et al., 2014; Salimans et al., 2016).\\nComputers and Geotechnics 183(2025)107177\\n4'), np.float32(-0.006238103)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i29', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='s arranged \\nin a U-Net framework (Ronneberger et al., 2015).\\nBeginning with CPT-like images of dimensions 512 √ó 32 the en-\\ncoder progressively downsamples the images through eight sequential \\nblocks. The initial four blocks are regular encoder blocks (RE), each \\nwith increasing numbers of filters (64, 128, 256, and 512), alongside \\nstandard batch normalisation to improve the stability of the network \\nby normalising the activations of each layer (Santurkar et al., 2018). \\nSubsequently, irregular encoder blocks (IE) are employed from the \\nfifth to the eighth block, with 512 filters each and standard batch \\nnormalisation. These blocks adjust the stride of the convolutional layer \\nto 1 √ó 2, which determines the step size of the filter‚Äôs movement across \\nthe input data, accommodating the schematisation 1:16 aspect ratio. \\nFollowing findings from Goodfellow et al. (2014), Isola et al.'), np.float32(-0.011897802)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i37', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content=', adhering \\nto best practices in the field of GAN training. During training, the \\nDiscriminator‚Äôs parameters are adjusted based on the results from the \\nloss function just like in the Generator.\\n2.4. The combined cGAN architecture\\nFor the compilation of the combined Generator and Discriminator \\nnetworks into the schemaGAN model, the Adam optimiser was also \\nused with a learning rate of 0.0002 and a beta value of 0.5 (Goodfellow \\net al., 2014; Salimans et al., 2016).\\nThe complete schemaGAN model boasts a total of 78,172,226 pa-\\nrameters representing the weights and biases of the network‚Äôs layers. \\nFrom these, a total of 67,003,137 are trainable parameters adjusted \\nduring the training process to optimise the model‚Äôs performance. The \\nremaining 11,169,089 parameters are non-trainable and contribute to \\nthe structure and functionality of the network without being influenced \\nby training.\\n2.5.'), np.float32(-0.015737534)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i20', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='orks: \\nthe Generator and the Discriminator. The Generator takes as input the \\nCPT-like data that acts as a condition \\ue00a, along with  a noise vector \\n\\ue012, and produces a schematisation of the subsurface. Meanwhile, the \\nDiscriminator is presented with a balanced mix of real schematisations \\nand schematisations generated by the Generator, each paired with its \\ncorresponding conditional CPT-like data, and it is tasked with distin-\\nguishing between real and fake schematisations. Depending on the \\nDiscriminator‚Äôs assessment, feedback is provided to either the Gener-\\nator, encouraging it to refine its generation of fake schematisations, \\nor to the Discriminator, prompting it to improve in  differentiating \\nbetween real and fake schematisations. This feedback mechanism is \\nintegrated into the networks through the utilisation of loss functions.'), np.float32(-0.016216278)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i27', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='etween the generated outputs and the ground \\ntruth, the Generator aims to create outputs that closely resemble the \\nreal data at a pixel level, thereby enhancing fidelity.\\nThe \\ue001 parameter in this Eq. (1) acts as a balancing factor, allowing \\nadjustments in the trade-off between image diversity and accuracy. A \\nsmaller \\ue001 encourages the Generator to generate more diverse outputs, \\nwhile a larger \\ue001 prioritises fidelity to the real data. This flexibility \\nenables fine-tuning of the Generator performance (Isola et al., 2017).\\n2.2. The generator architecture\\nThe Generator employs convolutional neural networks (CNNs), \\nspecifically a modified U-Net architecture, due to their efficacy in \\nhandling high-dimensional data (LeCun et al., 2015).'), np.float32(-0.0230757)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i11', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='type \\nof DL model consisting of two neural networks, which are trained \\nsimultaneously in a competitive setting to learn the underlying distri-\\nbution of the training data (Goodfellow et al., 2014). Unlike previous \\nmethods, GANs can generate new data points that closely mirror the \\nexisting data (Creswell et al., 2018; Goodfellow et al., 2020), a critical \\naspect for accurately capturing the complex relationships in subsurface \\nschematisation, where data is sparse and heterogeneous.'), np.float32(-0.038441062)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i21', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='Discriminator, prompting it to improve in  differentiating \\nbetween real and fake schematisations. This feedback mechanism is \\nintegrated into the networks through the utilisation of loss functions. \\nThrough multiple training cycles, the Generator progressively learns \\nto deceive the Discriminator, ultimately becoming a proper tool to \\ntranslate CPT-like data into accurate subsurface schematisations.\\n2.1. Objective function\\nThe objective function of SchemaGAN is a composite loss function \\nthat captures the training process illustrated in Fig. 1, and it is defined \\nas: \\n\\ue00d\\ue00c\\ue00e\\n\\ue006\\n\\ue00d\\ue009\\ue010\\n\\ue005\\n\\ue000\\ue00a\\ue006\\ue004\\ue008 (\\ue005,\\ue006) +\\ue001\\ue000 \\ue0071(\\ue006) (1)\\nComputers and Geotechnics 183(2025)107177\\n2'), np.float32(-0.040374517)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i19', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='twork architecture from Isola et al. (2017) \\n(i.e. the pi√ó2pi√ó method) is modified in order to perform translation \\ntasks from one-dimensional sparse CPT data to complete subsurface \\nschematisations. For the training of schemaGAN, a synthetic database \\nof geotechnical schematisations was generated. For each schematisa-\\ntion, an accompanying synthetic CPT image was generated as described \\nin Section 3.4 and will be referred  to as CPT-like data. Illustrated in \\nFig. 1, the training framework comprises two independent networks: \\nthe Generator and the Discriminator. The Generator takes as input the \\nCPT-like data that acts as a condition \\ue00a, along with  a noise vector \\n\\ue012, and produces a schematisation of the subsurface.'), np.float32(-0.053680897)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i30', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='2, which determines the step size of the filter‚Äôs movement across \\nthe input data, accommodating the schematisation 1:16 aspect ratio. \\nFollowing findings from Goodfellow et al. (2014), Isola et al. (2017) \\nlayer weights are initialised with a standard deviation of 0.02 for stable \\ntraining, and a leakyReLU activation function with an alpha of 0.2, \\nwhere alpha defines the slope of the function for negative input values. \\nThis helps to improve the flow of gradients and prevents neurons from \\nbecoming saturated. Following the last encoder block, a bottleneck is \\nformed by a convolutional 2D layer (Conv2D) with 512 filters, a kernel \\nsize of 4 √ó 4, and a stride of 2 √ó 2, compressing the image into a \\nlower-dimensional representation with a ReLU activation function. The \\ndecoder process mirrors the encoder‚Äôs structure symmetrically, starting \\nwith a regular decoder block (RD), followed by four irregular decoder \\nblocks (ID), and concluding with three additional RD blocks.'), np.float32(-0.05461514))]\n",
      "  self.vectorstore.similarity_search_with_relevance_scores(\n",
      "d:\\LLMs\\DLLMForge\\.venv\\lib\\site-packages\\langchain_core\\vectorstores\\base.py:1082: UserWarning: Relevance scores must be between 0 and 1, got [(Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i28', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='or architecture\\nThe Generator employs convolutional neural networks (CNNs), \\nspecifically a modified U-Net architecture, due to their efficacy in \\nhandling high-dimensional data (LeCun et al., 2015). U-Net is a neural \\nnetwork architecture commonly used for image segmentation tasks, \\ncharacterised by its encoder‚Äìdecoder structure with skip connections in \\na symmetric structure (Ronneberger et al., 2015), facilitating the con-\\nveyance of both local and global structural information for generating \\nhigh-quality images.\\nFig. 2 illustrates the general architecture of the Generator, which \\ncomprises regular and irregular encoder and decoder blocks arranged \\nin a U-Net framework (Ronneberger et al., 2015).\\nBeginning with CPT-like images of dimensions 512 √ó 32 the en-\\ncoder progressively downsamples the images through eight sequential \\nblocks.'), np.float32(0.16820514)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i38', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='optimise the model‚Äôs performance. The \\nremaining 11,169,089 parameters are non-trainable and contribute to \\nthe structure and functionality of the network without being influenced \\nby training.\\n2.5. Training and validation of the model\\nThe final model undergoes training for 200 epochs, where each \\nepoch represents one complete pass through the entire dataset. During \\ntraining, a batch size of 1 is utilised, meaning that each training \\niteration involves processing a single sample from the dataset. This \\napproach mitigates mode collapse, allows for precise gradient up-\\ndates, conserves memory, and is known for faster convergence in the \\nmodels (Goodfellow et al., 2014; Salimans et al., 2016).\\nComputers and Geotechnics 183(2025)107177\\n4'), np.float32(-0.006238103)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i29', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='s arranged \\nin a U-Net framework (Ronneberger et al., 2015).\\nBeginning with CPT-like images of dimensions 512 √ó 32 the en-\\ncoder progressively downsamples the images through eight sequential \\nblocks. The initial four blocks are regular encoder blocks (RE), each \\nwith increasing numbers of filters (64, 128, 256, and 512), alongside \\nstandard batch normalisation to improve the stability of the network \\nby normalising the activations of each layer (Santurkar et al., 2018). \\nSubsequently, irregular encoder blocks (IE) are employed from the \\nfifth to the eighth block, with 512 filters each and standard batch \\nnormalisation. These blocks adjust the stride of the convolutional layer \\nto 1 √ó 2, which determines the step size of the filter‚Äôs movement across \\nthe input data, accommodating the schematisation 1:16 aspect ratio. \\nFollowing findings from Goodfellow et al. (2014), Isola et al.'), np.float32(-0.011897802)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i37', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content=', adhering \\nto best practices in the field of GAN training. During training, the \\nDiscriminator‚Äôs parameters are adjusted based on the results from the \\nloss function just like in the Generator.\\n2.4. The combined cGAN architecture\\nFor the compilation of the combined Generator and Discriminator \\nnetworks into the schemaGAN model, the Adam optimiser was also \\nused with a learning rate of 0.0002 and a beta value of 0.5 (Goodfellow \\net al., 2014; Salimans et al., 2016).\\nThe complete schemaGAN model boasts a total of 78,172,226 pa-\\nrameters representing the weights and biases of the network‚Äôs layers. \\nFrom these, a total of 67,003,137 are trainable parameters adjusted \\nduring the training process to optimise the model‚Äôs performance. The \\nremaining 11,169,089 parameters are non-trainable and contribute to \\nthe structure and functionality of the network without being influenced \\nby training.\\n2.5.'), np.float32(-0.015737534)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i20', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='orks: \\nthe Generator and the Discriminator. The Generator takes as input the \\nCPT-like data that acts as a condition \\ue00a, along with  a noise vector \\n\\ue012, and produces a schematisation of the subsurface. Meanwhile, the \\nDiscriminator is presented with a balanced mix of real schematisations \\nand schematisations generated by the Generator, each paired with its \\ncorresponding conditional CPT-like data, and it is tasked with distin-\\nguishing between real and fake schematisations. Depending on the \\nDiscriminator‚Äôs assessment, feedback is provided to either the Gener-\\nator, encouraging it to refine its generation of fake schematisations, \\nor to the Discriminator, prompting it to improve in  differentiating \\nbetween real and fake schematisations. This feedback mechanism is \\nintegrated into the networks through the utilisation of loss functions.'), np.float32(-0.016216278)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i27', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='etween the generated outputs and the ground \\ntruth, the Generator aims to create outputs that closely resemble the \\nreal data at a pixel level, thereby enhancing fidelity.\\nThe \\ue001 parameter in this Eq. (1) acts as a balancing factor, allowing \\nadjustments in the trade-off between image diversity and accuracy. A \\nsmaller \\ue001 encourages the Generator to generate more diverse outputs, \\nwhile a larger \\ue001 prioritises fidelity to the real data. This flexibility \\nenables fine-tuning of the Generator performance (Isola et al., 2017).\\n2.2. The generator architecture\\nThe Generator employs convolutional neural networks (CNNs), \\nspecifically a modified U-Net architecture, due to their efficacy in \\nhandling high-dimensional data (LeCun et al., 2015).'), np.float32(-0.0230757)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i11', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='type \\nof DL model consisting of two neural networks, which are trained \\nsimultaneously in a competitive setting to learn the underlying distri-\\nbution of the training data (Goodfellow et al., 2014). Unlike previous \\nmethods, GANs can generate new data points that closely mirror the \\nexisting data (Creswell et al., 2018; Goodfellow et al., 2020), a critical \\naspect for accurately capturing the complex relationships in subsurface \\nschematisation, where data is sparse and heterogeneous.'), np.float32(-0.038441062)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i21', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='Discriminator, prompting it to improve in  differentiating \\nbetween real and fake schematisations. This feedback mechanism is \\nintegrated into the networks through the utilisation of loss functions. \\nThrough multiple training cycles, the Generator progressively learns \\nto deceive the Discriminator, ultimately becoming a proper tool to \\ntranslate CPT-like data into accurate subsurface schematisations.\\n2.1. Objective function\\nThe objective function of SchemaGAN is a composite loss function \\nthat captures the training process illustrated in Fig. 1, and it is defined \\nas: \\n\\ue00d\\ue00c\\ue00e\\n\\ue006\\n\\ue00d\\ue009\\ue010\\n\\ue005\\n\\ue000\\ue00a\\ue006\\ue004\\ue008 (\\ue005,\\ue006) +\\ue001\\ue000 \\ue0071(\\ue006) (1)\\nComputers and Geotechnics 183(2025)107177\\n2'), np.float32(-0.040374517)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i19', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='twork architecture from Isola et al. (2017) \\n(i.e. the pi√ó2pi√ó method) is modified in order to perform translation \\ntasks from one-dimensional sparse CPT data to complete subsurface \\nschematisations. For the training of schemaGAN, a synthetic database \\nof geotechnical schematisations was generated. For each schematisa-\\ntion, an accompanying synthetic CPT image was generated as described \\nin Section 3.4 and will be referred  to as CPT-like data. Illustrated in \\nFig. 1, the training framework comprises two independent networks: \\nthe Generator and the Discriminator. The Generator takes as input the \\nCPT-like data that acts as a condition \\ue00a, along with  a noise vector \\n\\ue012, and produces a schematisation of the subsurface.'), np.float32(-0.053680897)), (Document(id='SchemaGAN_ A conditional Generative Adversarial Network for geotechnical subsurface schematisation - 1-s2.0-S0266352X25001260-main.pdf_i30', metadata={'/Producer': 'cairo 1.18.0 (https://cairographics.org)', '/Creator': 'Mozilla Firefox 144.0.2', '/CreationDate': \"D:20251030112331+01'00\"}, page_content='2, which determines the step size of the filter‚Äôs movement across \\nthe input data, accommodating the schematisation 1:16 aspect ratio. \\nFollowing findings from Goodfellow et al. (2014), Isola et al. (2017) \\nlayer weights are initialised with a standard deviation of 0.02 for stable \\ntraining, and a leakyReLU activation function with an alpha of 0.2, \\nwhere alpha defines the slope of the function for negative input values. \\nThis helps to improve the flow of gradients and prevents neurons from \\nbecoming saturated. Following the last encoder block, a bottleneck is \\nformed by a convolutional 2D layer (Conv2D) with 512 filters, a kernel \\nsize of 4 √ó 4, and a stride of 2 √ó 2, compressing the image into a \\nlower-dimensional representation with a ReLU activation function. The \\ndecoder process mirrors the encoder‚Äôs structure symmetrically, starting \\nwith a regular decoder block (RD), followed by four irregular decoder \\nblocks (ID), and concluding with three additional RD blocks.'), np.float32(-0.05461514))]\n",
      "  self.vectorstore.similarity_search_with_relevance_scores(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting RAG evaluation...\n",
      "  üìä Evaluating context relevancy...\n",
      "  üìä Evaluating faithfulness...\n",
      "  üìä Evaluating answer relevancy...\n",
      "  üìä Evaluating context recall...\n",
      "Question: What is the network architecture based on?\n",
      "RAGAS Score: 0.625\n",
      "Answer Relevancy: 0.950\n",
      "Faithfulness: 0.800\n",
      "Context Recall: 0.000\n",
      "Context Relevancy: 0.400\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define test questions with ground truth answers\n",
    "TEST_QUESTIONS = [{\n",
    "    \"question\": \"Size of images produced by schemaGAN?\",\n",
    "    \"ground_truth\": \"The images produced by schemaGAN have a size of **512 √ó 32 pixels**.\"\n",
    "}, {\n",
    "    \"question\": \"What is the network architecture based on?\",\n",
    "    \"ground_truth\": \"the pix2pix method from Isola et al. (2017)\"\n",
    "}]\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RAGEvaluator(llm_provider=\"deltares\", deltares_llm=llm)\n",
    "\n",
    "results = []\n",
    "for q_data in TEST_QUESTIONS:\n",
    "    question = q_data[\"question\"]\n",
    "    ground_truth = q_data[\"ground_truth\"]\n",
    "    \n",
    "    # Generate answer\n",
    "    chat_result = llm.ask_with_retriever(question, retriever)\n",
    "    answer = chat_result.generations[0].message.content\n",
    "    answer = answer.split(\"</think>\")[-1].strip()  # Clean up response\n",
    "    \n",
    "    # Get retrieved contexts\n",
    "    retrieved_contexts = retriever.invoke(question)\n",
    "    \n",
    "    # Evaluate the RAG pipeline\n",
    "    evaluation = evaluator.evaluate_rag_pipeline(\n",
    "        question=question,\n",
    "        generated_answer=answer,\n",
    "        retrieved_contexts=retrieved_contexts,\n",
    "        ground_truth_answer=ground_truth\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'question': question,\n",
    "        'ground_truth': ground_truth,\n",
    "        'response': answer,\n",
    "        'context': retrieved_contexts,\n",
    "        'evaluation': evaluation\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"RAGAS Score: {evaluation.ragas_score:.3f}\")\n",
    "    print(f\"Answer Relevancy: {evaluation.answer_relevancy.score:.3f}\")\n",
    "    print(f\"Faithfulness: {evaluation.faithfulness.score:.3f}\")\n",
    "    print(f\"Context Recall: {evaluation.context_recall.score:.3f}\")\n",
    "    print(f\"Context Relevancy: {evaluation.context_relevancy.score:.3f}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658b8b3d",
   "metadata": {},
   "source": [
    "## Understanding Evaluation Metrics\n",
    "\n",
    "The RAG evaluation provides four key metrics:\n",
    "\n",
    "### 1. Context Relevancy (0-1)\n",
    "Measures how relevant the retrieved documents are to the question. \n",
    "\n",
    "- **Higher scores** = Better retrieval\n",
    "- **Low scores?** ‚Üí Check embedding model or vector store configuration\n",
    "\n",
    "### 2. Context Recall (0-1)\n",
    "Measures whether all necessary information was retrieved by comparing retrieved context with ground truth.\n",
    "\n",
    "- **Low scores?** ‚Üí Important documents may be missing\n",
    "- **Improvement:** Adjust chunk size, overlap, or try a different embedding model\n",
    "\n",
    "### 3. Faithfulness (0-1)\n",
    "Measures factual accuracy and absence of hallucinations. Checks if the answer is grounded in the retrieved context.\n",
    "\n",
    "- **Low scores?** ‚Üí LLM may be generating unsupported information\n",
    "- **Improvement:** Adjust LLM temperature or try a different model\n",
    "\n",
    "### 4. Answer Relevancy (0-1)\n",
    "Measures how directly the answer addresses the question. Penalizes verbose or off-topic responses.\n",
    "\n",
    "- **Low scores?** ‚Üí LLM may not be using retrieved context effectively\n",
    "- **Improvement:** Experiment with prompt engineering or different LLMs\n",
    "\n",
    "### RAGAS Score\n",
    "Overall score combining all metrics, providing a single measure of RAG system quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f769c63f",
   "metadata": {},
   "source": [
    "## Advanced Configuration\n",
    "\n",
    "Now let's explore some advanced techniques to optimize your RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198ce5d",
   "metadata": {},
   "source": [
    "### Optimizing Chunk Size\n",
    "\n",
    "Experiment with different chunk sizes based on your document types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad6aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For technical documents with detailed information\n",
    "chunker_technical = TextChunker(chunk_size=1500, overlap_size=300)\n",
    "\n",
    "# For shorter, conversational content\n",
    "chunker_short = TextChunker(chunk_size=500, overlap_size=100)\n",
    "\n",
    "# For very long documents\n",
    "chunker_long = TextChunker(chunk_size=2000, overlap_size=400)\n",
    "\n",
    "print(\"Different chunkers configured for various document types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7a1499",
   "metadata": {},
   "source": [
    "### Using Different Embedding Models\n",
    "\n",
    "Try different embedding models for better performance. Visit the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) for more options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More capable but larger model\n",
    "# model = LangchainHFEmbeddingModel(\n",
    "#     model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "# )\n",
    "\n",
    "# Multilingual model\n",
    "# model = LangchainHFEmbeddingModel(\n",
    "#     model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "# )\n",
    "\n",
    "# Domain-specific model (for scientific papers)\n",
    "# model = LangchainHFEmbeddingModel(\n",
    "#     model_name=\"sentence-transformers/allenai-specter\"\n",
    "# )\n",
    "\n",
    "print(\"Alternative embedding models available (commented out)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5490a56d",
   "metadata": {},
   "source": [
    "### Improving Retrieval\n",
    "\n",
    "Fine-tune retrieval parameters for different use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0285b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For high precision (fewer but more relevant results)\n",
    "retriever_precision = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        \"score_threshold\": 0.3,  # Higher threshold\n",
    "        \"k\": 5  # Fewer results\n",
    "    },\n",
    ")\n",
    "\n",
    "# For high recall (more results, potentially less relevant)\n",
    "retriever_recall = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 20  # More results\n",
    "    },\n",
    ")\n",
    "\n",
    "# For diverse results using Maximum Marginal Relevance\n",
    "retriever_mmr = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 10,\n",
    "        \"lambda_mult\": 0.7  # Balance between relevance and diversity\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Different retriever configurations created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbeebb8",
   "metadata": {},
   "source": [
    "### Docling Automatic Chunker (Advanced)\n",
    "\n",
    "If documents are not well structured, use the automatic chunker from Docling. This chunker uses a language model to create semantically meaningful chunks based on content and structure.\n",
    "\n",
    "**Benefits:**\n",
    "- More semantically meaningful chunks\n",
    "- Better handling of document structure\n",
    "- Improved retrieval performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a897a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This replaces the previous chunking code\n",
    "from langchain_docling import DoclingLoader\n",
    "from docling.chunking import HybridChunker\n",
    "from langchain_docling.loader import ExportType\n",
    "\n",
    "# Find all PDF files in the directory\n",
    "pdfs = list(data_dir.glob(\"*.pdf\"))\n",
    "global_chunks = []\n",
    "\n",
    "for pdf_path in pdfs:\n",
    "    loader = DoclingLoader(\n",
    "        file_path=pdf_path,\n",
    "        export_type=ExportType.DOC_CHUNKS,\n",
    "        chunker=HybridChunker(\n",
    "            tokenizer=model.embeddings.model_name,\n",
    "            chunk_size=512,        # Max length supported by MiniLM\n",
    "            chunk_overlap=50       # Some overlap for better context\n",
    "        )\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    global_chunks.extend(docs)\n",
    "\n",
    "print(f\"Total chunks generated with Docling: {len(global_chunks)}\")\n",
    "\n",
    "# Create FAISS vector store\n",
    "index = faiss.IndexFlatL2(len(model.embed(\"test\")))\n",
    "vector_store_docling = FAISS(\n",
    "    embedding_function=model.embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore({}),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "vector_store_docling.add_documents(global_chunks)\n",
    "\n",
    "# Query the vector store\n",
    "query_embedding = vector_store_docling.similarity_search_with_score(\n",
    "    query=\"Size of images for schema GAN in pixels\", \n",
    "    k=5\n",
    ")\n",
    "print(\"\\nDocling Query result:\")\n",
    "for i, (doc, score) in enumerate(query_embedding, 1):\n",
    "    print(f\"{i}. Score: {score:.4f} - {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d852bbb4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've built a complete RAG pipeline. Here's what you learned:\n",
    "\n",
    "‚úÖ **Document Processing**: Load and chunk PDFs efficiently  \n",
    "‚úÖ **Embeddings**: Generate vector embeddings with open source models  \n",
    "‚úÖ **Vector Storage**: Store and retrieve embeddings with FAISS  \n",
    "‚úÖ **LLM Integration**: Use open source LLMs for answer generation  \n",
    "‚úÖ **Evaluation**: Measure RAG performance with RAGAS metrics  \n",
    "‚úÖ **Optimization**: Fine-tune chunking, retrieval, and embeddings  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Experiment with different embedding models\n",
    "2. Try various chunk sizes and overlap settings\n",
    "3. Test different retriever configurations\n",
    "4. Evaluate with your own documents and questions\n",
    "5. Explore Docling for improved document structure handling\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Start simple**: Use default configurations first\n",
    "- **Iterate**: Adjust parameters based on evaluation metrics\n",
    "- **Document-specific**: Optimize for your specific document types\n",
    "- **Balance**: Trade-off between speed, accuracy, and resource usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

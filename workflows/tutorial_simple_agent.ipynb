{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e7eaf77",
   "metadata": {},
   "source": [
    "# Building a Simple Agent with DLLMForge\n",
    "\n",
    "This tutorial demonstrates how to build a simple tool-using agent using DLLMForge. You will learn how to configure different LLM providers (Azure OpenAI, OpenAI, Mistral, or Deltares-hosted models), create custom tools, and build an agent that can perform calculations and answer questions about pizza prices.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The simple agent tutorial consists of several key components:\n",
    "\n",
    "1. **Environment Setup**: Configure API keys and credentials for different LLM providers\n",
    "2. **Tool Creation**: Define custom tools using the DLLMForge `@tool` decorator\n",
    "3. **Agent Initialization**: Create a SimpleAgent with your chosen LLM provider\n",
    "4. **Tool Integration**: Add tools to the agent and compile the agent\n",
    "5. **Query Processing**: Test the agent with various queries and observe tool routing\n",
    "6. **Provider Switching**: Change between different LLM providers without code changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6376e42c",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Python environment with the project requirements installed\n",
    "2. A `.env` file in your project root with provider credentials (see below)\n",
    "3. Optional: IPython/Jupyter if you want to display the LangGraph diagram\n",
    "\n",
    "### Environment Setup\n",
    "\n",
    "Create or update your `.env` with the variables for the providers you plan to use.\n",
    "\n",
    "**Azure OpenAI (default in DLLMForge examples):**\n",
    "```bash\n",
    "AZURE_OPENAI_ENDPOINT=https://your-azure-endpoint\n",
    "AZURE_OPENAI_API_KEY=your_azure_openai_api_key\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=your_deployment_name\n",
    "AZURE_OPENAI_API_VERSION=2024-12-01-preview\n",
    "```\n",
    "\n",
    "**OpenAI:**\n",
    "```bash\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "OPENAI_MODEL_NAME=gpt-4o  # or gpt-4o, etc.\n",
    "```\n",
    "\n",
    "**Mistral:**\n",
    "```bash\n",
    "MISTRAL_API_KEY=your_mistral_api_key\n",
    "MISTRAL_MODEL_NAME=mistral-large-latest\n",
    "```\n",
    "\n",
    "**Deltares-hosted (no API key; requires Deltares network/VPN):**\n",
    "```bash\n",
    "# No keys required; you will specify base_url and model at runtime\n",
    "DELTARES_BASE_URL=https://chat-api.directory.intra\n",
    "DELTARES_MODEL_NAME=llama3.1:70b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2198011",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Modules\n",
    "\n",
    "Start by loading environment variables and importing all necessary components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "753ab560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file for API keys and endpoints\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Import dllmforge simple agent and tool decorator\n",
    "from dllmforge.agent_core import SimpleAgent, tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6c9e23",
   "metadata": {},
   "source": [
    "## Step 2: Define Basic Math Tools\n",
    "\n",
    "Create custom tools using the `@tool` decorator. These tools will be available to the agent for performing calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ea4622d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dllmforge.agent_core:Registering DLLMForge tool: add\n",
      "INFO:dllmforge.agent_core:Registering DLLMForge tool: subtract\n",
      "INFO:dllmforge.agent_core:Registering DLLMForge tool: multiply\n",
      "INFO:dllmforge.agent_core:Registering DLLMForge tool: divide\n",
      "INFO:dllmforge.agent_core:Registering DLLMForge tool: subtract\n",
      "INFO:dllmforge.agent_core:Registering DLLMForge tool: multiply\n",
      "INFO:dllmforge.agent_core:Registering DLLMForge tool: divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Math tools defined: add, subtract, multiply, divide\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers together.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def subtract(a: float, b: float) -> float:\n",
    "    \"\"\"Subtract two numbers from each other.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def divide(a: float, b: float) -> float:\n",
    "    \"\"\"Divide two numbers.\"\"\"\n",
    "    return a / b\n",
    "\n",
    "print(\"‚úÖ Math tools defined: add, subtract, multiply, divide\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e4b086",
   "metadata": {},
   "source": [
    "## Step 3: Define Pizza Pricing Tool\n",
    "\n",
    "Create a domain-specific tool that retrieves pizza prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1441c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dllmforge.agent_core:Registering DLLMForge tool: get_pizza_price\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pizza pricing tool defined\n",
      "Available pizzas: {'margherita': 12.99, 'pepperoni': 15.99, 'vegetarian': 14.99, 'supreme': 17.99}\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def get_pizza_price(pizza_type: str) -> float:\n",
    "    \"\"\"Get the price of a pizza type.\"\"\"\n",
    "    prices = {\n",
    "        \"margherita\": 12.99,\n",
    "        \"pepperoni\": 15.99,\n",
    "        \"vegetarian\": 14.99,\n",
    "        \"supreme\": 17.99\n",
    "    }\n",
    "    return prices.get(pizza_type.lower(), 10.99)\n",
    "\n",
    "print(\"‚úÖ Pizza pricing tool defined\")\n",
    "print(\"Available pizzas:\", {\"margherita\": 12.99, \"pepperoni\": 15.99, \"vegetarian\": 14.99, \"supreme\": 17.99})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e9121",
   "metadata": {},
   "source": [
    "## Step 4: Define LLM-Powered Summary Tool\n",
    "\n",
    "Create a tool that uses an LLM to generate conversational summaries of results. This demonstrates how tools can call LLMs internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fdda414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dllmforge.agent_core:Registering DLLMForge tool: make_summary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summary tool defined (uses LLM internally)\n"
     ]
    }
   ],
   "source": [
    "# Provider switch for the summary tool: 'azure' or 'deltares'\n",
    "PROVIDER = (os.getenv(\"PIZZA_LLM_PROVIDER\") or \"azure\").lower()\n",
    "deltares_available = False  # toggled true after connectivity check\n",
    "shared_llm = None  # Reused when using Deltares for both agent and summary\n",
    "\n",
    "\n",
    "@tool\n",
    "def make_summary(question: str, result: str) -> str:\n",
    "    \"\"\"Use the configured LLM to create a concise, conversational summary.\n",
    "\n",
    "    Args:\n",
    "        question: The original user question.\n",
    "        result: The computed or retrieved result to summarize.\n",
    "\n",
    "    Returns:\n",
    "        A short, user-friendly summary generated by the LLM.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "        if PROVIDER == \"deltares\" and deltares_available:\n",
    "            # Use a shared Deltares LLM instance\n",
    "            global shared_llm\n",
    "            if shared_llm is None:\n",
    "                from dllmforge.LLMs.Deltares_LLMs import DeltaresOllamaLLM\n",
    "                base_url = os.getenv(\"DELTARES_BASE_URL\", \"https://chat-api.directory.intra\")\n",
    "                model_name = os.getenv(\"DELTARES_MODEL_NAME\", \"llama3.1:70b\")\n",
    "                shared_llm = DeltaresOllamaLLM(base_url=base_url, model_name=model_name)\n",
    "            invoke_fn = shared_llm.invoke\n",
    "        else:\n",
    "            # Default to Azure OpenAI via DLLMForge's LangchainAPI\n",
    "            from dllmforge.langchain_api import LangchainAPI\n",
    "            llm_api = LangchainAPI()  # defaults use env to configure Azure\n",
    "            invoke_fn = llm_api.llm.invoke\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=(\n",
    "                    \"You are a helpful assistant. Create a concise, friendly summary of the provided result \"\n",
    "                    \"in the context of the question. Mention all of the tools that you used\"\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Question:\\n{question}\\n\\nResult:\\n{result}\\n\\nPlease return a brief, conversational summary (1-3 sentences).\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "        response = invoke_fn(messages)\n",
    "        return getattr(response, \"content\", str(response))\n",
    "    except Exception as e:\n",
    "        return f\"Could not generate summary: {e}\"\n",
    "\n",
    "print(\"‚úÖ Summary tool defined (uses LLM internally)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc2e712",
   "metadata": {},
   "source": [
    "## Step 5: Create the Agent\n",
    "\n",
    "Initialize a SimpleAgent with clear instructions. The agent will use the configured LLM provider (Azure OpenAI by default, or Deltares if configured)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "462e6205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dllmforge.agent_core:Simple agent initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent created with Azure OpenAI (default)\n",
      "\n",
      "üìã System Message:\n",
      "You are a helpful assistant that can do math and tell you about pizza prices. Only use the tools, do not try to do maths in your head.\n"
     ]
    }
   ],
   "source": [
    "# Create agent based on provider configuration\n",
    "if PROVIDER == \"deltares\":\n",
    "    # Build and reuse a Deltares LLM for both routing and summarisation\n",
    "    from dllmforge.LLMs.Deltares_LLMs import DeltaresOllamaLLM\n",
    "    base_url = os.getenv(\"DELTARES_BASE_URL\", \"https://chat-api.directory.intra\")\n",
    "    model_name = os.getenv(\"DELTARES_MODEL_NAME\", \"llama3.1:70b\")\n",
    "    shared_llm = DeltaresOllamaLLM(base_url=base_url, model_name=model_name)\n",
    "\n",
    "    # Connectivity check with a tiny ping; fallback to Azure if it fails\n",
    "    try:\n",
    "        from langchain_core.messages import HumanMessage\n",
    "        _ = shared_llm.invoke([HumanMessage(content=\"ping\")])\n",
    "        deltares_available = True\n",
    "        print(\"‚úÖ Connected to Deltares LLM\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Deltares LLM not reachable ({e}). Falling back to Azure for routing and summary.\")\n",
    "\n",
    "    if deltares_available:\n",
    "        routing_system = (\n",
    "            \"You are a helpful assistant that can do math and tell you about pizza prices.\"\n",
    "            \" When you need a tool, respond ONLY with a JSON object like {\\\"tool\\\": \\\"<name>\\\", \\\"args\\\": {...}}.\"\n",
    "            \" Use exact tool names: add, multiply, divide, subtract, get_pizza_price, make_summary.\"\n",
    "        )\n",
    "        agent = SimpleAgent(\n",
    "            routing_system,\n",
    "            temperature=0.1,\n",
    "            llm=shared_llm,\n",
    "            enable_text_tool_routing=True,\n",
    "            max_tool_iterations=4,\n",
    "        )\n",
    "    else:\n",
    "        agent = SimpleAgent(\n",
    "            \"You are a helpful assistant that can do math and tell you about pizza prices. Only use the tools, do not try to do maths in your head.\",\n",
    "            temperature=0.1\n",
    "        )\n",
    "else:\n",
    "    agent = SimpleAgent(\n",
    "        \"You are a helpful assistant that can do math and tell you about pizza prices. Only use the tools, do not try to do maths in your head.\",\n",
    "        temperature=0.1\n",
    "    )\n",
    "    print(\"‚úÖ Agent created with Azure OpenAI (default)\")\n",
    "\n",
    "print(\"\\nüìã System Message:\")\n",
    "print(agent.system_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9100001b",
   "metadata": {},
   "source": [
    "## Step 6: Add Tools to the Agent\n",
    "\n",
    "Register all the tools we've created with the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb5c6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dllmforge.agent_core:Added tool: make_summary\n",
      "INFO:dllmforge.agent_core:Added tool: divide\n",
      "INFO:dllmforge.agent_core:Added tool: multiply\n",
      "INFO:dllmforge.agent_core:Added tool: add\n",
      "INFO:dllmforge.agent_core:Added tool: subtract\n",
      "INFO:dllmforge.agent_core:Added tool: get_pizza_price\n",
      "INFO:dllmforge.agent_core:Added tool: divide\n",
      "INFO:dllmforge.agent_core:Added tool: multiply\n",
      "INFO:dllmforge.agent_core:Added tool: add\n",
      "INFO:dllmforge.agent_core:Added tool: subtract\n",
      "INFO:dllmforge.agent_core:Added tool: get_pizza_price\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All tools added to agent:\n",
      "  ‚Ä¢ Math tools: add, subtract, multiply, divide\n",
      "  ‚Ä¢ Domain tools: get_pizza_price\n",
      "  ‚Ä¢ LLM tools: make_summary\n"
     ]
    }
   ],
   "source": [
    "agent.add_tool(make_summary)\n",
    "agent.add_tool(divide)\n",
    "agent.add_tool(multiply)\n",
    "agent.add_tool(add)\n",
    "agent.add_tool(subtract)\n",
    "agent.add_tool(get_pizza_price)\n",
    "\n",
    "print(\"‚úÖ All tools added to agent:\")\n",
    "print(\"  ‚Ä¢ Math tools: add, subtract, multiply, divide\")\n",
    "print(\"  ‚Ä¢ Domain tools: get_pizza_price\")\n",
    "print(\"  ‚Ä¢ LLM tools: make_summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058194fc",
   "metadata": {},
   "source": [
    "## Step 7: Compile the Agent\n",
    "\n",
    "Compile the agent workflow. This creates the LangGraph workflow that handles tool routing and execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "264550a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dllmforge.agent_core:Added node: agent\n",
      "INFO:dllmforge.agent_core:Added node: tools\n",
      "INFO:dllmforge.agent_core:Added edge: __start__ -> agent\n",
      "INFO:dllmforge.agent_core:Added conditional edge from: agent\n",
      "INFO:dllmforge.agent_core:Added edge: tools -> agent\n",
      "INFO:dllmforge.agent_core:Simple workflow created with human interaction support\n",
      "INFO:dllmforge.agent_core:Added node: tools\n",
      "INFO:dllmforge.agent_core:Added edge: __start__ -> agent\n",
      "INFO:dllmforge.agent_core:Added conditional edge from: agent\n",
      "INFO:dllmforge.agent_core:Added edge: tools -> agent\n",
      "INFO:dllmforge.agent_core:Simple workflow created with human interaction support\n",
      "INFO:dllmforge.agent_core:Workflow compiled successfully\n",
      "INFO:dllmforge.agent_core:Workflow compiled successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent compiled and ready to use!\n",
      "\n",
      "============================================================\n",
      "üçï PIZZA AGENT READY\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "agent.compile()\n",
    "\n",
    "print(\"‚úÖ Agent compiled and ready to use!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üçï PIZZA AGENT READY\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370fcd4",
   "metadata": {},
   "source": [
    "## Testing the Agent\n",
    "\n",
    "Now let's test the agent with various queries to see how it uses tools to solve problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5585cc9",
   "metadata": {},
   "source": [
    "### Test Case 1: Simple Pizza Calculation\n",
    "\n",
    "This query asks about the total price of two pizzas. The agent should:\n",
    "1. Use `get_pizza_price` to get the pepperoni price (15.99)\n",
    "2. Use `get_pizza_price` to get the margherita price (12.99)\n",
    "3. Use `add` to calculate the total (28.98)\n",
    "4. Use `make_summary` to provide a friendly response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "568b8322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TEST 1: What is the total price of a pepperoni pizza and a margherita pizza?\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "PROCESSING: What is the total price of a pepperoni pizza and a margherita pizza?\n",
      "============================================================\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the total price of a pepperoni pizza and a margherita pizza?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_pizza_price (call_0HeoftdYERu5EstXAEzExwXt)\n",
      " Call ID: call_0HeoftdYERu5EstXAEzExwXt\n",
      "  Args:\n",
      "    pizza_type: pepperoni\n",
      "  get_pizza_price (call_TenETx7n8ighc7Emn5oodwvE)\n",
      " Call ID: call_TenETx7n8ighc7Emn5oodwvE\n",
      "  Args:\n",
      "    pizza_type: margherita\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_pizza_price\n",
      "\n",
      "12.99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (call_6N1br5ZE0Q0dY213fGrwEgro)\n",
      " Call ID: call_6N1br5ZE0Q0dY213fGrwEgro\n",
      "  Args:\n",
      "    a: 15.99\n",
      "    b: 12.99\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "28.98\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  make_summary (call_JzlRzdkED2NzjjG3HG6OCjUD)\n",
      " Call ID: call_JzlRzdkED2NzjjG3HG6OCjUD\n",
      "  Args:\n",
      "    question: What is the total price of a pepperoni pizza and a margherita pizza?\n",
      "    result: The total price for a pepperoni pizza ($15.99) and a margherita pizza ($12.99) is $28.98.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: make_summary\n",
      "\n",
      "If you order a pepperoni pizza and a margherita pizza together, the total comes to $28.98. I used the menu prices for both pizzas to calculate this.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "If you order a pepperoni pizza and a margherita pizza together, the total comes to $28.98. I used the menu prices for both pizzas to calculate this.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What is the total price of a pepperoni pizza and a margherita pizza?\"\n",
    "print(f\"üß™ TEST 1: {query1}\")\n",
    "print(\"-\" * 60)\n",
    "agent.process_query(query1, stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c885a7",
   "metadata": {},
   "source": [
    "### Test Case 2: Complex Fraction Calculation\n",
    "\n",
    "This query involves fractions and cost-sharing. The agent should:\n",
    "1. Get the pepperoni price (15.99)\n",
    "2. Get the margherita price (12.99)\n",
    "3. Calculate 2/3 of the pepperoni price (friend's share)\n",
    "4. Calculate 1/2 of the margherita price (friend's share)\n",
    "5. Add the two amounts to get the total the friend owes\n",
    "6. Generate a conversational summary with the Tikkie amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71407a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TEST 2: My friend ate 2/3 of a pepperoni pizza and I ate 1/2 of a margherita pizza and I paid for both pizzas. How much should I Tikkie her for her share?\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "PROCESSING: My friend ate 2/3 of a pepperoni pizza and I ate 1/2 of a margherita pizza and I paid for both pizzas. How much should I Tikkie her for her share?\n",
      "============================================================\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "My friend ate 2/3 of a pepperoni pizza and I ate 1/2 of a margherita pizza and I paid for both pizzas. How much should I Tikkie her for her share?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_pizza_price (call_siF6j32Yozu6fxJ5hrplcOLp)\n",
      " Call ID: call_siF6j32Yozu6fxJ5hrplcOLp\n",
      "  Args:\n",
      "    pizza_type: pepperoni\n",
      "  get_pizza_price (call_VTi4gId6BLAEznFHEFZ0uhMO)\n",
      " Call ID: call_VTi4gId6BLAEznFHEFZ0uhMO\n",
      "  Args:\n",
      "    pizza_type: margherita\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_pizza_price\n",
      "\n",
      "12.99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_DuJsxrnUadjkDZ81pdYNw9hm)\n",
      " Call ID: call_DuJsxrnUadjkDZ81pdYNw9hm\n",
      "  Args:\n",
      "    a: 15.99\n",
      "    b: 0.6667\n",
      "  multiply (call_oPfUZaFaRHgTlZj9yh6soT2G)\n",
      " Call ID: call_oPfUZaFaRHgTlZj9yh6soT2G\n",
      "  Args:\n",
      "    a: 12.99\n",
      "    b: 0.5\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "6.495\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (call_JW9K0Ci3HSJoW1O1ITQ5xEZr)\n",
      " Call ID: call_JW9K0Ci3HSJoW1O1ITQ5xEZr\n",
      "  Args:\n",
      "    a: 10.660533\n",
      "    b: 6.495\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "17.155533\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  divide (call_38QGsUJYrGUs26mBwL0GBBSK)\n",
      " Call ID: call_38QGsUJYrGUs26mBwL0GBBSK\n",
      "  Args:\n",
      "    a: 17.155533\n",
      "    b: 2\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: divide\n",
      "\n",
      "8.5777665\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  make_summary (call_SixuHoIn7VAXkq0vs9mYpt8T)\n",
      " Call ID: call_SixuHoIn7VAXkq0vs9mYpt8T\n",
      "  Args:\n",
      "    question: How much should I Tikkie my friend for her share if she ate 2/3 of a pepperoni pizza and I ate 1/2 of a margherita pizza, and I paid for both?\n",
      "    result: You should Tikkie your friend ‚Ç¨8.58 for her share.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: make_summary\n",
      "\n",
      "You should Tikkie your friend ‚Ç¨8.58 for her share of the pizzas. I used basic fraction math to split the costs based on how much each of you ate. Let me know if you need a breakdown of the calculation!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You should Tikkie your friend ‚Ç¨8.58 for her share of the pizzas. This splits the cost fairly based on how much each of you ate. Let me know if you want to see the calculation details!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query2 = \"My friend ate 2/3 of a pepperoni pizza and I ate 1/2 of a margherita pizza and I paid for both pizzas. How much should I Tikkie her for her share?\"\n",
    "print(f\"üß™ TEST 2: {query2}\")\n",
    "print(\"-\" * 60)\n",
    "agent.process_query(query2, stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f77254",
   "metadata": {},
   "source": [
    "### Test Case 3: Multiple Pizza Types\n",
    "\n",
    "Let's try a query with different pizza types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14d542a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TEST 3: If I order 2 supreme pizzas and 1 vegetarian pizza, what's the total cost?\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "PROCESSING: If I order 2 supreme pizzas and 1 vegetarian pizza, what's the total cost?\n",
      "============================================================\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "If I order 2 supreme pizzas and 1 vegetarian pizza, what's the total cost?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_pizza_price (call_npcopaVRmLWqtZZxPrCtuSQk)\n",
      " Call ID: call_npcopaVRmLWqtZZxPrCtuSQk\n",
      "  Args:\n",
      "    pizza_type: supreme\n",
      "  get_pizza_price (call_vQxrqKEq6TzjuwSsa6aKUQHF)\n",
      " Call ID: call_vQxrqKEq6TzjuwSsa6aKUQHF\n",
      "  Args:\n",
      "    pizza_type: vegetarian\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_pizza_price\n",
      "\n",
      "14.99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_xfxyMqJgLOn0vnJaJXGmkagC)\n",
      " Call ID: call_xfxyMqJgLOn0vnJaJXGmkagC\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 17.99\n",
      "  multiply (call_uSSCY2numUduIUSOUoRhmrHe)\n",
      " Call ID: call_uSSCY2numUduIUSOUoRhmrHe\n",
      "  Args:\n",
      "    a: 1\n",
      "    b: 14.99\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "14.99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (call_xALsCB9gS1aXMnl0dWPkrs63)\n",
      " Call ID: call_xALsCB9gS1aXMnl0dWPkrs63\n",
      "  Args:\n",
      "    a: 35.98\n",
      "    b: 14.99\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "50.97\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  make_summary (call_0z731pOhHBT2ZTHCBWk7fBhV)\n",
      " Call ID: call_0z731pOhHBT2ZTHCBWk7fBhV\n",
      "  Args:\n",
      "    question: If I order 2 supreme pizzas and 1 vegetarian pizza, what's the total cost?\n",
      "    result: The total cost for 2 supreme pizzas and 1 vegetarian pizza is $50.97.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: make_summary\n",
      "\n",
      "If you order 2 supreme pizzas and 1 vegetarian pizza, your total comes to $50.97. I used the menu prices to calculate this for you!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "If you order 2 supreme pizzas and 1 vegetarian pizza, your total comes to $50.97. I used the menu prices to calculate this for you!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query3 = \"If I order 2 supreme pizzas and 1 vegetarian pizza, what's the total cost?\"\n",
    "print(f\"üß™ TEST 3: {query3}\")\n",
    "print(\"-\" * 60)\n",
    "agent.process_query(query3, stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9726857",
   "metadata": {},
   "source": [
    "### Test Case 4: Simple Math\n",
    "\n",
    "Test the agent's pure calculation abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f1e7c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TEST 4: What is 25.5 divided by 3, then multiplied by 2?\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "PROCESSING: What is 25.5 divided by 3, then multiplied by 2?\n",
      "============================================================\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is 25.5 divided by 3, then multiplied by 2?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  divide (call_Q8rTaVfMi0j0eQHAFo4FITV9)\n",
      " Call ID: call_Q8rTaVfMi0j0eQHAFo4FITV9\n",
      "  Args:\n",
      "    a: 25.5\n",
      "    b: 3\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: divide\n",
      "\n",
      "8.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_1sgNK50ZYaRUe5igm24Dqbf4)\n",
      " Call ID: call_1sgNK50ZYaRUe5igm24Dqbf4\n",
      "  Args:\n",
      "    a: 8.5\n",
      "    b: 2\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "17.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  make_summary (call_PwFG1OkWRoAo3hL6AUwdKblS)\n",
      " Call ID: call_PwFG1OkWRoAo3hL6AUwdKblS\n",
      "  Args:\n",
      "    question: What is 25.5 divided by 3, then multiplied by 2?\n",
      "    result: The answer is 17.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: make_summary\n",
      "\n",
      "To solve this, I first divided 25.5 by 3 to get 8.5, then multiplied that by 2 to get 17. I used basic arithmetic operations (division and multiplication) to find the answer. So, 25.5 divided by 3 and then multiplied by 2 equals 17!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "25.5 divided by 3 is 8.5, and when you multiply that by 2, you get 17. So, the answer is 17!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query4 = \"What is 25.5 divided by 3, then multiplied by 2?\"\n",
    "print(f\"üß™ TEST 4: {query4}\")\n",
    "print(\"-\" * 60)\n",
    "agent.process_query(query4, stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede138e5",
   "metadata": {},
   "source": [
    "## Try Your Own Query\n",
    "\n",
    "Now it's your turn! Try asking the agent your own questions about pizza prices or calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "645e6c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your query: How much would 3 margherita pizzas and 2 pepperoni pizzas cost?\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "PROCESSING: How much would 3 margherita pizzas and 2 pepperoni pizzas cost?\n",
      "============================================================\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How much would 3 margherita pizzas and 2 pepperoni pizzas cost?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_pizza_price (call_CmcxkBKM82Yt8eFNa5inddmj)\n",
      " Call ID: call_CmcxkBKM82Yt8eFNa5inddmj\n",
      "  Args:\n",
      "    pizza_type: margherita\n",
      "  get_pizza_price (call_a1Bi5LUIgYHhIHZt3WlqI65e)\n",
      " Call ID: call_a1Bi5LUIgYHhIHZt3WlqI65e\n",
      "  Args:\n",
      "    pizza_type: pepperoni\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_pizza_price\n",
      "\n",
      "15.99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_g1hGBLeVQCBmlbOqXz6pBXCu)\n",
      " Call ID: call_g1hGBLeVQCBmlbOqXz6pBXCu\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 12.99\n",
      "  multiply (call_Nd4J8RYfCvJwX5xglvjkxOmP)\n",
      " Call ID: call_Nd4J8RYfCvJwX5xglvjkxOmP\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 15.99\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "31.98\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (call_gIzOet4ZDWjdmzdP4v0WpzAc)\n",
      " Call ID: call_gIzOet4ZDWjdmzdP4v0WpzAc\n",
      "  Args:\n",
      "    a: 38.97\n",
      "    b: 31.98\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "70.95\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  make_summary (call_6g4Htdcy5gCbrWSSDYnqhPA0)\n",
      " Call ID: call_6g4Htdcy5gCbrWSSDYnqhPA0\n",
      "  Args:\n",
      "    question: How much would 3 margherita pizzas and 2 pepperoni pizzas cost?\n",
      "    result: The total cost for 3 margherita pizzas and 2 pepperoni pizzas is $70.95.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: make_summary\n",
      "\n",
      "Sure! The total for 3 margherita pizzas and 2 pepperoni pizzas comes to $70.95. I used the provided pricing information to calculate this for you.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openaicoastal.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The total for 3 margherita pizzas and 2 pepperoni pizzas comes to $70.95. Let me know if you need prices for other combinations!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try your own query here\n",
    "your_query = \"How much would 3 margherita pizzas and 2 pepperoni pizzas cost?\"\n",
    "print(f\"Your query: {your_query}\")\n",
    "print(\"-\" * 60)\n",
    "agent.process_query(your_query, stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec8a6fb",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "‚úÖ **Simple Tool Creation**: Use the `@tool` decorator to create custom tools with minimal boilerplate\n",
    "\n",
    "‚úÖ **Flexible LLM Providers**: Switch between Azure OpenAI, OpenAI, Mistral, or Deltares-hosted models easily\n",
    "\n",
    "‚úÖ **Automatic Tool Routing**: The agent automatically decides which tools to use based on the query\n",
    "\n",
    "‚úÖ **Composable Tools**: Tools can call LLMs internally (like the `make_summary` tool)\n",
    "\n",
    "‚úÖ **Streaming Output**: Use `stream=True` to see tool calls and results in real-time\n",
    "\n",
    "‚úÖ **One-Line Compilation**: Simple `.compile()` call creates a complete LangGraph workflow\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Add more domain-specific tools for your use case\n",
    "- Explore the advanced agent tutorial for conditional routing and specialized nodes\n",
    "- Try different LLM providers to compare cost, latency, and quality\n",
    "- Build RAG systems with document retrieval tools\n",
    "- Create multi-agent systems for complex workflows\n",
    "\n",
    "This tutorial provides a solid foundation for building tool-using agents with DLLMForge! üçï‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

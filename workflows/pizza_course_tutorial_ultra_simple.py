"""
Pizza Course Tutorial - Ultra Simple DLLMForge Version
======================================================
Super high-level and simple - demonstrates the power of DLLMForge abstraction
"""

# Load environment variables from .env file for API keys and endpoints
from dotenv import load_dotenv

load_dotenv()
import os
import sys
from pathlib import Path

# Ensure local dllmforge package is used when running from the workflows directory
PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# Provider switch for the summary tool: 'azure' or 'deltares'
PROVIDER = (os.getenv("PIZZA_LLM_PROVIDER") or "azure").lower()
deltares_available = False  # toggled true after connectivity check

# Import dllmforge simple agent and tool decorator
from dllmforge.agent_core import SimpleAgent, tool

# ============================================================================
# 1. DEFINE TOOLS
# ============================================================================


# Start with some basic maths tools
@tool
def add(a: float, b: float) -> float:
    """Add two numbers together."""
    return a + b


@tool
def multiply(a: float, b: float) -> float:
    """Divide two numbers."""
    return a * b


@tool
def divide(a: float, b: float) -> float:
    """Divide two numbers."""
    return a / b


@tool
def subtract(a: float, b: float) -> float:
    """Subtract two numbers from each other."""
    return a - b


# Now some pizza tools, get the price of a pizza type
@tool
def get_pizza_price(pizza_type: str) -> float:
    """Get the price of a pizza type."""
    prices = {"margherita": 12.99, "pepperoni": 15.99, "vegetarian": 14.99, "supreme": 17.99}
    return prices.get(pizza_type.lower(), 10.99)


shared_llm = None  # Reused when using Deltares for both agent and summary


# Now a tool to make a summary of the result
@tool
def make_summary(question: str, result: str) -> str:
    """Use the configured LLM to create a concise, conversational summary.

    Args:
        question: The original user question.
        result: The computed or retrieved result to summarize.

    Returns:
        A short, user-friendly summary generated by the LLM.
    """
    try:
        from langchain_core.messages import SystemMessage, HumanMessage

        if PROVIDER == "deltares" and deltares_available:
            # Use a shared Deltares LLM instance
            global shared_llm
            if shared_llm is None:
                from dllmforge.LLMs.Deltares_LLMs import DeltaresOllamaLLM
                base_url = os.getenv("DELTARES_BASE_URL", "https://chat-api.directory.intra")
                model_name = os.getenv("DELTARES_MODEL_NAME", "llama3.1:70b")
                shared_llm = DeltaresOllamaLLM(base_url=base_url, model_name=model_name)
            invoke_fn = shared_llm.invoke
        else:
            # Default to Azure OpenAI via DLLMForge's LangchainAPI
            from dllmforge.langchain_api import LangchainAPI
            llm_api = LangchainAPI()  # defaults use env to configure Azure
            invoke_fn = llm_api.llm.invoke

        messages = [
            SystemMessage(
                content=("You are a helpful assistant. Create a concise, friendly summary of the provided result "
                         "in the context of the question. Mention all of the tools that you used")),
            HumanMessage(content=(
                f"Question:\n{question}\n\nResult:\n{result}\n\nPlease return a brief, conversational summary (1-3 sentences)."
            )),
        ]
        response = invoke_fn(messages)
        return getattr(response, "content", str(response))
    except Exception as e:
        return f"Could not generate summary: {e}"


# ============================================================================
# 2. CREATE AGENT, give a some clear instructions to it
# ============================================================================

if PROVIDER == "deltares":
    # Build and reuse a Deltares LLM for both routing and summarisation
    from dllmforge.LLMs.Deltares_LLMs import DeltaresOllamaLLM
    base_url = os.getenv("DELTARES_BASE_URL", "https://chat-api.directory.intra")
    model_name = os.getenv("DELTARES_MODEL_NAME", "llama3.1:70b")
    shared_llm = DeltaresOllamaLLM(base_url=base_url, model_name=model_name)

    # Connectivity check with a tiny ping; fallback to Azure if it fails
    try:
        from langchain_core.messages import HumanMessage
        _ = shared_llm.invoke([HumanMessage(content="ping")])
        deltares_available = True
    except Exception as e:
        print(f"Deltares LLM not reachable ({e}). Falling back to Azure for routing and summary.")

    if deltares_available:
        routing_system = (
            "You are a helpful assistant that can do math and tell you about pizza prices."
            " When you need a tool, respond ONLY with a JSON object like {\"tool\": \"<name>\", \"args\": {...}}."
            " Use exact tool names: add, multiply, divide, subtract, get_pizza_price, make_summary.")
        agent = SimpleAgent(
            routing_system,
            temperature=0.1,
            llm=shared_llm,
            enable_text_tool_routing=True,
            max_tool_iterations=4,
        )
    else:
        agent = SimpleAgent(
            "You are a helpful assistant that can do math and tell you about pizza prices. Only use the tools, do not try to do maths in your head. ",
            temperature=0.1)
else:
    agent = SimpleAgent(
        "You are a helpful assistant that can do math and tell you about pizza prices. Only use the tools, do not try to do maths in your head. ",
        temperature=0.1)

# ============================================================================
# 3. ADD TOOLS
# ============================================================================

agent.add_tool(make_summary)
agent.add_tool(divide)
agent.add_tool(multiply)
agent.add_tool(add)
agent.add_tool(subtract)
agent.add_tool(get_pizza_price)

# ============================================================================
# 4. COMPILE - One line!
# ============================================================================

agent.compile()

# ============================================================================
# 6. TEST IT!
# ============================================================================

if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("TESTING THE ULTRA SIMPLE AGENT")
    print("=" * 60)

    # Test cases
    test_queries = [
        "What is the total price of a pepperoni pizza and a margherita pizza?",
        "My friend ate 2/3 of a pepperoni pizza and I ate 1/2 of a margherita pizza and I paid for both pizzas. How much should I Tikkie her for share?"
    ]

    for i, query in enumerate(test_queries, 1):
        print(f"\nðŸ§ª TEST {i}: {query}")
        print("-" * 40)
        try:
            # Use stream=True to see tool calls
            agent.process_query(query, stream=True)
        except Exception as e:
            print(f"Error: {e}")
